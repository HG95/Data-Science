{"./":{"url":"./","title":"Introduction","keywords":"","body":"Introduction Update time： 2020-07-16 "},"数学基础/":{"url":"数学基础/","title":"数学基础","keywords":"","body":"数学基础 import numpy as np import pandas as pd Update time： 2020-07-17 "},"数学基础/数值计算基础/":{"url":"数学基础/数值计算基础/","title":"数值计算基础","keywords":"","body":"数值计算基础 Update time： 2020-07-16 "},"数学基础/数值计算基础/梯度与梯度下降.html":{"url":"数学基础/数值计算基础/梯度与梯度下降.html","title":"梯度与梯度下降","keywords":"","body":"梯度与梯度下降 一、导数 一张图读懂导数与微分： 如果忘记了导数微分的概念，基本看着这张图就能全部想起来。 导数定义如下： f′(x0)=limΔx→0ΔyΔx=limΔx→0f(x0+Δx)−f(x0)Δx f^{\\prime}\\left(x_{0}\\right)=\\lim _{\\Delta x \\rightarrow 0} \\frac{\\Delta y}{\\Delta x}=\\lim _{\\Delta x \\rightarrow 0} \\frac{f\\left(x_{0}+\\Delta x\\right)-f\\left(x_{0}\\right)}{\\Delta x} f​′​​(x​0​​)=​Δx→0​lim​​​Δx​​Δy​​=​Δx→0​lim​​​Δx​​f(x​0​​+Δx)−f(x​0​​)​​ 反映的是函数 y=f(x)y=f(x)y=f(x) 在某一点处沿 xxx 轴正方向的变化率。再强调一遍，是函数 f(x)f(x)f(x) 在 xxx 轴上某一点处沿着 xxx 轴正方向的变化率/变化趋势。直观地看，也就是在 xxx 轴上某一点处，如果 f′(x)>0f^{'}(x)>0f​​′​​​​(x)>0，说明 f(x)f(x)f(x) 的函数值在 xxx 点沿 xxx 轴正方向是趋于增加的；如果 f′(x)0f^{'}(x)f​​′​​​​(x)0 ，说明 f(x)f(x)f(x) 的函数值在 x 点沿 x 轴正方向是趋于减少的。 这里补充上图中的 Δy,dy\\Delta y,dyΔy,dy 等符号的意义及关系如下： Δx:x\\Delta x:xΔx:x 的变化量； dx:xdx:xdx:x 的变化量 Δx\\Delta xΔx 趋于0 时，则记作微元 dxdxdx ； Δy:Δy=f(x0+Δx)−f(x0)\\Delta y:\\Delta y=f(x0+\\Delta x)-f(x0)Δy:Δy=f(x0+Δx)−f(x0) ，是函数的增量； dy:dy=f′(x0)dxdy:dy=f'(x0)dxdy:dy=f​′​​(x0)dx ，是切线的增量； 当Δx→0\\Delta x \\to 0Δx→0 时，dydydy 与 Δy\\Delta yΔy 都是无穷小，dydydy 是 Δy\\Delta yΔy 的主部，即 Δy=dy+o(Δx)\\Delta y=dy+o(\\Delta x)Δy=dy+o(Δx) . 二、导数和偏导数 偏导数的定义如下： ∂∂xjf(x0,x1,…,xn)=limΔx→0ΔyΔx=limΔx→0f(x0,…,xj+Δx,…,xn)−f(x0,…,xj,…,xn)Δx \\begin{aligned} \\frac{\\partial}{\\partial x_{j}} f\\left(x_{0}, x_{1}, \\ldots, x_{n}\\right) &=\\lim _{\\Delta x \\rightarrow 0} \\frac{\\Delta y}{\\Delta x}\\\\ &=\\lim _{\\Delta x \\rightarrow 0} \\frac{f\\left(x_{0}, \\ldots, x_{j}+\\Delta x, \\ldots, x_{n}\\right)-f\\left(x_{0}, \\ldots, x_{j}, \\ldots, x_{n}\\right)}{\\Delta x} \\end{aligned} ​​∂x​j​​​​∂​​f(x​0​​,x​1​​,…,x​n​​)​​​​=​Δx→0​lim​​​Δx​​Δy​​​=​Δx→0​lim​​​Δx​​f(x​0​​,…,x​j​​+Δx,…,x​n​​)−f(x​0​​,…,x​j​​,…,x​n​​)​​​​ 可以看到，导数与偏导数本质是一致的，都是当自变量的变化量趋于0时，函数值的变化量与自变量变化量比值的极限。直观地说，偏导数也就是函数在某一点上沿坐标轴正方向的的变化率。 区别在于： 导数，指的是一元函数中，函数 y=f(x)y=f(x)y=f(x) 在某一点处沿x轴正方向的变化率； 偏导数，指的是多元函数中，函数 y=f(x1,x2,…,xn)y=f\\left(x 1, x 2, \\ldots, xn\\right)y=f(x1,x2,…,xn) 在某一点处沿某一坐标轴正(x1,x2,…,xn)(x 1, x 2, \\ldots, x n)(x1,x2,…,xn)方向的变化率。 三、导数与方向导数 方向导数的定义如下： ∂∂xjf(x0,x1,…,xn)=limΔx→0ΔyΔx=limΔx→0f(x0,…,xj+Δx,…,xn)−f(x0,…,xj,…,xn)Δx \\begin{aligned} \\frac{\\partial}{\\partial x_{j}} f\\left(x_{0}, x_{1}, \\ldots, x_{n}\\right)&=\\lim _{\\Delta x \\rightarrow 0} \\frac{\\Delta y}{\\Delta x}\\\\&=\\lim _{\\Delta x \\rightarrow 0} \\frac{f\\left(x_{0}, \\ldots, x_{j}+\\Delta x, \\ldots, x_{n}\\right)-f\\left(x_{0}, \\ldots, x_{j}, \\ldots, x_{n}\\right)}{\\Delta x} \\end{aligned} ​​∂x​j​​​​∂​​f(x​0​​,x​1​​,…,x​n​​)​​​​=​Δx→0​lim​​​Δx​​Δy​​​=​Δx→0​lim​​​Δx​​f(x​0​​,…,x​j​​+Δx,…,x​n​​)−f(x​0​​,…,x​j​​,…,x​n​​)​​​​ 在前面导数和偏导数的定义中，均是沿坐标轴正方向讨论函数的变化率。那么当我们讨论函数沿任意方向的变化率时，也就引出了方向导数的定义，即：某一点在某一趋近方向上的导数值。 通俗的解释是： 我们不仅要知道函数在坐标轴正方向上的变化率（即偏导数），而且还要设法求得函数在其他特定方向上的变化率。而方向导数就是函数在其他特定方向上的变化率。 四、导数与梯度 标量函数 f:Rn↦Rf: \\mathbb{R}^{n} \\mapsto \\mathbb{R}f:R​n​​↦R 的梯度表示为：∇f\\nabla f∇f 或 gradfgrad\\;fgradf，其中 ∇\\nabla ∇（nabla)表示向量微分算子。 梯度定义如下： 函数在某一点的梯度是这样一个向量，它的方向与取得最大方向导数的方向一致，而它的模为方向导数的最大值。 一般的多元函数的梯度： f=f(x0,x1,x2,x3,⋯,xn)f=f\\left(x_{0},x_{1}, x_{2}, x_{3}, \\cdots, x_{n}\\right)f=f(x​0​​,x​1​​,x​2​​,x​3​​,⋯,x​n​​) 且其偏导数存在，则 fff 的梯度 gradfgrad\\; fgradf 为一向量函数。 gradf(x0,x1,…,xn)=(∂f∂x0,…,∂f∂xj,…,∂f∂xn) grad\\; f\\left(x_{0}, x_{1}, \\ldots, x_{n}\\right)=\\left(\\frac{\\partial f}{\\partial x_{0}}, \\ldots, \\frac{\\partial f}{\\partial x_{j}}, \\ldots, \\frac{\\partial f}{\\partial x_{n}}\\right) gradf(x​0​​,x​1​​,…,x​n​​)=(​∂x​0​​​​∂f​​,…,​∂x​j​​​​∂f​​,…,​∂x​n​​​​∂f​​) 4.1 对向量的梯度 以 n×1实向量x为变元的实标量函数 f(x)f(x)f(x)相对于x的梯度为一n×1列向量x，定义为: ∇xf(x)= def [∂f(x)∂x1,∂f(x)∂x2,⋯,∂f(x)∂xn]T=∂f(x)∂x \\nabla_{x} f(x) \\stackrel{\\text { def }}{=}\\left[\\frac{\\partial f(x)}{\\partial x_{1}}, \\frac{\\partial f(x)}{\\partial x_{2}}, \\cdots, \\frac{\\partial f(x)}{\\partial x_{n}}\\right]^{T}=\\frac{\\partial f(x)}{\\partial x} ∇​x​​f(x)​=​ def ​​[​∂x​1​​​​∂f(x)​​,​∂x​2​​​​∂f(x)​​,⋯,​∂x​n​​​​∂f(x)​​]​T​​=​∂x​​∂f(x)​​ m 维行向量函数 f(x)=[f1(x),f2(x),⋯,fm(x)]{\\displaystyle {\\mathbf {f}}({\\mathbf {x}})=[f_{1}({\\mathbf {x}}),f_{2}({\\mathbf {x}}),\\cdots ,f_{m}({\\mathbf {x}})]}f(x)=[f​1​​(x),f​2​​(x),⋯,f​m​​(x)] 相对于 n 维实向量x的梯度为一n×m矩阵，定义为 4.2 对矩阵的梯度 实标量函数f(A) {\\mathbf {f}}({\\mathbf {A}})f(A) 相对于m×n实矩阵A的梯度为一m×n矩阵，简称梯度矩阵，定义为 梯度的提出只为回答一个问题： 函数在变量空间的某一点处，沿着哪一个方向有最大的变化率？ 这里注意三点： 　1）梯度是一个向量，即有方向有大小； 　2）梯度的方向是最大方向导数的方向； 　3）梯度的值是最大方向导数的值。 五、导数与向量 　提问：导数与偏导数与方向导数是向量么？ 　向量的定义是有方向（direction）有大小（magnitude）的量。 　从前面的定义可以这样看出，偏导数和方向导数表达的是函数在某一点沿某一方向的变化率，也是具有方向和大小的。因此从这个角度来理解，我们也可以把偏导数和方向导数看作是一个向量，向量的方向就是变化率的方向，向量的模，就是变化率的大小。 　那么沿着这样一种思路，就可以如下理解梯度： 　梯度即函数在某一点最大的方向导数，函数沿梯度方向函数有最大的变化率。 六、梯度下降法 6.1 梯度下降的直观解释 首先来看看梯度下降的一个直观的解释。比如我们在一座大山上的某处位置，由于我们不知道怎么下山，于是决定走一步算一步，也就是在每走到一个位置的时候，求解当前位置的梯度，沿着梯度的负方向，也就是当前最陡峭的位置向下走一步，然后继续求解当前位置梯度，向这一步所在位置沿着最陡峭最易下山的位置走一步。这样一步步的走下去，一直走到觉得我们已经到了山脚。当然这样走下去，有可能我们不能走到山脚，而是到了某一个局部的山峰低处。 从上面的解释可以看出，梯度下降不一定能够找到全局的最优解，有可能是一个局部最优解。当然，如果损失函数是凸函数，梯度下降法得到的解就一定是全局最优解。 6.2 梯度下降的相关概念 在详细了解梯度下降的算法之前，我们先看看相关的一些概念。 步长（Learning rate） 步长决定了在梯度下降迭代的过程中，每一步沿梯度负方向前进的长度。用上面下山的例子，步长就是在当前这一步所在位置沿着最陡峭最易下山的位置走的那一步的长度。 特征（feature） 指的是样本中输入部分，比如2个单特征的样本 (x(0),y(0)),(x(1),y(1))(x^{(0)},y^{(0)}),(x^{(1)},y^{(1)})(x​(0)​​,y​(0)​​),(x​(1)​​,y​(1)​​) ,则第一个样本特征为 x(0)x^{(0)}x​(0)​​ , 第一个样本输出为 y(0)y^{(0)}y​(0)​​ 。 假设函数（hypothesis function） 在监督学习中，为了拟合输入样本，而使用的假设函数，记为 hθ(x) h_{\\theta}(x)h​θ​​(x) 。比如对于单个特征的m个样本 (x(i),y(i))(i=1,2,...m)(x^{(i)},y^{(i)})(i=1,2,...m)(x​(i)​​,y​(i)​​)(i=1,2,...m) , 可以采用拟合函数如下：hθ(x)=θ0+θ1xh_{\\theta}(x) = \\theta_0+\\theta_1xh​θ​​(x)=θ​0​​+θ​1​​x . 损失函数（loss function） 为了评估模型拟合的好坏，通常用损失函数来度量拟合的程度。损失函数极小化，意味着拟合程度最好，对应的模型参数即为最优参数。在线性回归中，损失函数通常为样本输出和假设函数的差取平方。比如对于 m 个样本 (xi,yi)(i=1,2,...m)(x_i,y_i)(i=1,2,...m)(x​i​​,y​i​​)(i=1,2,...m) , 采用线性回归，损失函数为： J(θ0,θ1)=∑i=1m(hθ(xi)−yi)2 J(\\theta_0, \\theta_1) = \\sum\\limits_{i=1}^{m}(h_\\theta(x_i) - y_i)^2 J(θ​0​​,θ​1​​)=​i=1​∑​m​​(h​θ​​(x​i​​)−y​i​​)​2​​ ​ 其中 xix_ix​i​​ 表示第 iii 个样本特征，yiy_iy​i​​ 表示第 iii 个样本对应的输出， 为假设hθ(xi)h_{\\theta}\\left(x_{i}\\right)h​θ​​(x​i​​) 函数。 6.3 梯度下降的详细算法 　　梯度下降法的算法可以有代数法和矩阵法（也称向量法）两种表示，如果对矩阵分析不熟悉，则代数法更加容易理解。不过矩阵法更加的简洁，且由于使用了矩阵，实现逻辑更加的一目了然。这里先介绍代数法，后介绍矩阵法。 梯度下降法的代数方式描述 1.先决条件： 确认优化模型的假设函数和损失函数。 比如对于线性回归，假设函数表示为 hθ(x1,x2,...xn)=θ0+θ1x1+...+θnxnh_\\theta(x_1, x_2, ...x_n) = \\theta_0 + \\theta_{1}x_1 + ... + \\theta_{n}x_{n}h​θ​​(x​1​​,x​2​​,...x​n​​)=θ​0​​+θ​1​​x​1​​+...+θ​n​​x​n​​ , 其中 θi(i=0,1,2...n)\\theta_i (i = 0,1,2... n)θ​i​​(i=0,1,2...n) 为模型参数，xi(i=0,1,2...n)x_i (i = 0,1,2... n)x​i​​(i=0,1,2...n) 为每个样本的n个特征值。这个表示可以简化，我们增加一个特征 x0=1x_0=1x​0​​=1 ，这样 hθ(x0,x1,...xn)=∑i=0nθixih_\\theta(x_0, x_1, ...x_n) = \\sum\\limits_{i=0}^{n}\\theta_{i}x_{i}h​θ​​(x​0​​,x​1​​,...x​n​​)=​i=0​∑​n​​θ​i​​x​i​​ 。 同样是线性回归，对应于上面的假设函数，损失函数为： J(θ0,θ1...,θn)=12m∑j=0m(hθ(x0(j),x1(j),...xn(j))−yj)2 J(\\theta_0, \\theta_1..., \\theta_n) = \\frac{1}{2m}\\sum\\limits_{j=0}^{m}(h_\\theta(x_0^{(j)}, x_1^{(j)}, ...x_n^{(j)}) - y_j)^2 J(θ​0​​,θ​1​​...,θ​n​​)=​2m​​1​​​j=0​∑​m​​(h​θ​​(x​0​(j)​​,x​1​(j)​​,...x​n​(j)​​)−y​j​​)​2​​ 2.算法相关参数初始化 主要是初始化 θ0,θ1...,θn\\theta_0, \\theta_1..., \\theta_nθ​0​​,θ​1​​...,θ​n​​ ,算法终止距离 ϵ\\epsilonϵ 以及步长 α\\alphaα 。在没有任何先验知识的时候，我喜欢将所有的 θ\\thetaθ 初始化为0， 将步长初始化为 1。在调优的时候再优化。 3.算法过程 1）确定当前位置的损失函数的梯度，对于 θi\\theta_iθ​i​​ ,其梯度表达式如下： ∂∂θiJ(θ0,θ1...,θn) \\frac{\\partial}{\\partial\\theta_i}J(\\theta_0, \\theta_1..., \\theta_n) ​∂θ​i​​​​∂​​J(θ​0​​,θ​1​​...,θ​n​​) 2）用步长乘以损失函数的梯度，得到当前位置下降的距离，即 α∂∂θiJ(θ0,θ1...,θn)\\alpha\\frac{\\partial}{\\partial\\theta_i}J(\\theta_0, \\theta_1..., \\theta_n)α​∂θ​i​​​​∂​​J(θ​0​​,θ​1​​...,θ​n​​) 对应于前面登山例子中的某一步。 3）确定是否所有的 θi\\theta_iθ​i​​ , 梯度下降的距离都小于 ϵ\\epsilonϵ , 如果小于 ϵ\\epsilonϵ 则算法终止，当前所有的 θi(i=0,1,...n)\\theta_i(i=0,1,...n)θ​i​​(i=0,1,...n) 即为最终结果。否则进入步骤4. 4）更新所有的 θ\\thetaθ，对于θi\\theta_iθ​i​​ ，其更新表达式如下。更新完毕后继续转入步骤1. θi=θi−α∂∂θiJ(θ0,θ1...,θn) \\theta_i = \\theta_i - \\alpha\\frac{\\partial}{\\partial\\theta_i}J(\\theta_0, \\theta_1..., \\theta_n) θ​i​​=θ​i​​−α​∂θ​i​​​​∂​​J(θ​0​​,θ​1​​...,θ​n​​) 下面用线性回归的例子来具体描述梯度下降。假设我们的样本是 : (x1(0),x2(0),...xn(0),y0),(x1(1),x2(1),...xn(1),y1),...(x1(m),x2(m),...xn(m),ym)(x_1^{(0)}, x_2^{(0)}, ...x_n^{(0)}, y_0), (x_1^{(1)}, x_2^{(1)}, ...x_n^{(1)},y_1), ... (x_1^{(m)}, x_2^{(m)}, ...x_n^{(m)}, y_m)(x​1​(0)​​,x​2​(0)​​,...x​n​(0)​​,y​0​​),(x​1​(1)​​,x​2​(1)​​,...x​n​(1)​​,y​1​​),...(x​1​(m)​​,x​2​(m)​​,...x​n​(m)​​,y​m​​) , 损失函数如前面先决条件所述： J(θ0,θ1...,θn)=12m∑j=0m(hθ(x0(j),x1(j),...xn(j))−yj)2 J(\\theta_0, \\theta_1..., \\theta_n) = \\frac{1}{2m}\\sum\\limits_{j=0}^{m}(h_\\theta(x_0^{(j)}, x_1^{(j)}, ...x_n^{(j)})- y_j)^2 J(θ​0​​,θ​1​​...,θ​n​​)=​2m​​1​​​j=0​∑​m​​(h​θ​​(x​0​(j)​​,x​1​(j)​​,...x​n​(j)​​)−y​j​​)​2​​ 则在算法过程步骤 1 中对于θi\\theta_iθ​i​​ 的偏导数计算如下： ∂∂θiJ(θ0,θ1...,θn)=1m∑j=0m(hθ(x0(j),x1(j),...xn(j))−yj)xi(j) \\frac{\\partial}{\\partial\\theta_i}J(\\theta_0, \\theta_1..., \\theta_n)= \\frac{1}{m}\\sum\\limits_{j=0}^{m}(h_\\theta(x_0^{(j)}, x_1^{(j)}, ...x_n^{(j)}) - y_j)x_i^{(j)} ​∂θ​i​​​​∂​​J(θ​0​​,θ​1​​...,θ​n​​)=​m​​1​​​j=0​∑​m​​(h​θ​​(x​0​(j)​​,x​1​(j)​​,...x​n​(j)​​)−y​j​​)x​i​(j)​​ 由于样本中没有 x0x_0x​0​​上式中令所有的 x0jx_0^{j}x​0​j​​ 为1. 步骤4中 θi\\theta_iθ​i​​ 的更新表达式如下： θi=θi−α1m∑j=0m(hθ(x0(j),x1(j),...xnj)−yj)xi(j) \\theta_i = \\theta_i - \\alpha\\frac{1}{m}\\sum\\limits_{j=0}^{m}(h_\\theta(x_0^{(j)}, x_1^{(j)}, ...x_n^{j}) - y_j)x_i^{(j)} θ​i​​=θ​i​​−α​m​​1​​​j=0​∑​m​​(h​θ​​(x​0​(j)​​,x​1​(j)​​,...x​n​j​​)−y​j​​)x​i​(j)​​ 从这个例子可以看出当前点的梯度方向是由所有的样本决定的，加 1m\\frac{1}{m}​m​​1​​ 是为了好理解。由于步长也为常数，他们的乘机也为常数，所以这里 α1m\\alpha\\frac{1}{m}α​m​​1​​ 可以用一个常数表示。 梯度下降法的矩阵方式描述 这一部分主要讲解梯度下降法的矩阵方式表述，相对于3.3.1的代数法，要求有一定的矩阵分析的基础知识，尤其是矩阵求导的知识。 1.先决条件： 和3.3.1类似， 需要确认优化模型的假设函数和损失函数。对于线性回归，假设函数 hθ(x1,x2,...xn)=θ0+θ1x1+...+θnxn h_\\theta(x_1, x_2, ...x_n) = \\theta_0 + \\theta_{1}x_1 + ... + \\theta_{n}x_{n} h​θ​​(x​1​​,x​2​​,...x​n​​)=θ​0​​+θ​1​​x​1​​+...+θ​n​​x​n​​ 的矩阵表达方式为：hθ(X)=Xθh_\\mathbf{\\theta}(\\mathbf{X}) = \\mathbf{X\\theta}h​θ​​(X)=Xθ , 其中， 假设函数 hθ(X)h_\\mathbf{\\theta}(\\mathbf{X})h​θ​​(X) 为 m×1m\\times1m×1 的向量 , θ\\thetaθ 为(n+1)×1(n+1) \\times 1(n+1)×1 的向量 , 里面有n+1个代数法的模型参数。X\\mathbf{X}X 为 m×(n+1)m\\times (n+1)m×(n+1) 维的矩阵。m 代表样本的个数，n+1代表样本的特征数。 2.算法相关参数初始化 θ\\thetaθ 向量可以初始化为默认值，或者调优后的值。算法终止距离 ϵ\\epsilonϵ ，步长 α\\alphaα 和3.3.1比没有变化。 3.算法过程： 1）确定当前位置的损失函数的梯度，对于 θ\\thetaθ 向量,其梯度表达式如下： ∂∂θJ(θ) \\frac{\\partial}{\\partial\\mathbf\\theta}J(\\mathbf\\theta) ​∂θ​​∂​​J(θ) 2）用步长乘以损失函数的梯度，得到当前位置下降的距离，即 α∂∂θJ(θ)\\alpha\\frac{\\partial}{\\partial\\theta}J(\\theta)α​∂θ​​∂​​J(θ) 对应于前面登山例子中的某一步。 3）确定 θ\\thetaθ 向量里面的每个值,梯度下降的距离都小于 ϵ\\epsilonϵ ，如果小于ϵ\\epsilonϵ 则算法终止，当前 θ\\thetaθ 向量即为最终结果。否则进入步骤4. 4）更新 θ\\thetaθ 向量，其更新表达式如下。更新完毕后继续转入步骤1. θ=θ−α∂∂θJ(θ) \\mathbf\\theta= \\mathbf\\theta - \\alpha\\frac{\\partial}{\\partial\\theta}J(\\mathbf\\theta) θ=θ−α​∂θ​​∂​​J(θ) 线性回归的例子来描述具体的算法过程。 损失函数对于 θ\\thetaθ 向量的偏导数计算如下： ∂∂θJ(θ)=XT(Xθ−Y) \\frac{\\partial}{\\partial\\mathbf\\theta}J(\\mathbf\\theta) = \\mathbf{X}^T(\\mathbf{X\\theta} - \\mathbf{Y}) ​∂θ​​∂​​J(θ)=X​T​​(Xθ−Y) 步骤4中 θ\\thetaθ 向量的更新表达式如下：θ=θ−αXT(Xθ−Y)\\mathbf\\theta= \\mathbf\\theta - \\alpha\\mathbf{X}^T(\\mathbf{X\\theta} - \\mathbf{Y})θ=θ−αX​T​​(Xθ−Y) 对于3.3.1的代数法，可以看到矩阵法要简洁很多。这里面用到了矩阵求导链式法则，和两个矩阵求导的公式。 这里面用到了矩阵求导链式法则，和两个个矩阵求导的公式。 公式1：∂∂x(xTx)=2xx\\frac{\\partial}{\\partial\\mathbf{x}}(\\mathbf{x^Tx}) =2\\mathbf{x}\\;\\;x ​∂x​​∂​​(x​T​​x)=2xx 为向量 公式2：∇Xf(AX+B)=AT∇Yf,Y=AX+B,f(Y) \\nabla_Xf(AX+B) = A^T\\nabla_Yf,\\;\\; Y=AX+B,\\;\\;f(Y)∇​X​​f(AX+B)=A​T​​∇​Y​​f,Y=AX+B,f(Y)为标量 6.4 梯度下降的算法调优 在使用梯度下降时，需要进行调优。哪些地方需要调优呢？ 算法的步长选择 : 在前面的算法描述中，我提到取步长为1，但是实际上取值取决于数据样本，可以多取一些值，从大到小，分别运行算法，看看迭代效果，如果损失函数在变小，说明取值有效，否则要增大步长。前面说了。步长太大，会导致迭代过快，甚至有可能错过最优解。步长太小，迭代速度太慢，很长时间算法都不能结束。所以算法的步长需要多次运行后才能得到一个较为优的值。 算法参数的初始值选择。 初始值不同，获得的最小值也有可能不同，因此梯度下降求得的只是局部最小值；当然如果损失函数是凸函数则一定是最优解。由于有局部最优解的风险，需要多次用不同初始值运行算法，关键损失函数的最小值，选择损失函数最小化的初值。 归一化。由于样本不同特征的取值范围不一样，可能导致迭代很慢，为了减少特征取值的影响，可以对特征数据归一化，也就是对于每个特征 x，求出它的期望 x‾\\overline{x}​x​​​ 和标准差 std(x)，然后转化为： x−x‾std(x) \\frac{x - \\overline{x}}{std(x)} ​std(x)​​x−​x​​​​​ 这样特征的新期望为0，新方差为1，迭代速度可以大大加快。 6.5 梯度下降法求极值 求下列函数的极小值: f(x)=x4−3∗x3+2⇒f′(x)=4x3−9x2 f(x)=x^{4}-3 * x^{3}+2 \\Rightarrow f^{\\prime}(x)=4 x^{3}-9 x^{2} f(x)=x​4​​−3∗x​3​​+2⇒f​′​​(x)=4x​3​​−9x​2​​ # From calculation, it is expected that the local minimum occurs at x=9/4 cur_x = 6 # The algorithm starts at x=6 gamma = 0.01 # step size multiplier precision = 0.00001 previous_step_size = cur_x def df(x): return 4 * x**3 - 9 * x**2 while previous_step_size > precision: prev_x = cur_x cur_x += -gamma * df(prev_x) previous_step_size = abs(cur_x - prev_x) print(\"The local minimum occurs at %f\" % cur_x) # The local minimum occurs at 2.249965 七、梯度下降法与最小二乘法 “机器学习”中有六个经典算法，其中就包括“最小二乘法”和“梯度下降法”，前者用于“搜索最小误差”，后者用于“用最快的速度搜索”，二者常常配合使用。代码演示如下： # y = mx + b # m is slope, b is y-intercept def compute_error_for_line_given_points(b, m, coordinates): totalerror = 0 for i in range(0, len(coordinates)): x = coordinates[i][0] y = coordinates[i][1] totalerror += (y - (m * x + b)) ** 2 return totalerror / float(len(coordinates)) # example compute_error_for_line_given_points(1, 2, [[3, 6], [6, 9], [12, 18]]) # 22.0 以上就是用“最小二乘法”来计算误差，当输入为 (1,2)(1,2) 时，输出为 22.0 很显然，最小二乘法需要不停地调整（试验）输入来找到一个最小误差。而应用“梯度下降法”，可以加快这个“试验”的过程。   以上面这段程序为例，误差是斜率 m 和常数 b 的二元函数，可以表示为 e=g(m,b) e=g(m, b) e=g(m,b)  那么，对最小二乘法的参数调优就转变为了求这个二元函数的极值问题，也就是说可以应用“梯度下降法”了。 “梯度下降法”可以用于搜索函数的局部极值，如下，求下列函数的局部极小值 f(x)=x5−2x3−2 f(x)=x^{5}-2 x^{3}-2 f(x)=x​5​​−2x​3​​−2 分析：这是一个一元连续函数，且可导，其导函数是： f′(x)=5x4−6x2 f^{\\prime}(x)=5 x^{4}-6 x^{2} f​′​​(x)=5x​4​​−6x​2​​ 根据“一阶导数极值判别法”：若函数 f(x)f(x)f(x) 可导，且 f′(x)f'(x)f​′​​(x) 在 x0x_0x​0​​ 的两侧异号，则 x0x_0x​0​​ 是 f(x)f(x)f(x) 的极值点。那么，怎么找到这个 x0x_0x​0​​ 呢？   很简单，只需要沿斜率（导数值）的反方向逐步移动即可，如下图：导数为负时，沿x轴正向移动；导数为正时，沿x轴负方向移动。 current_x = 0.5 # the algorithm starts at x=0.5 learning_rate = 0.01 # step size multiplier num_iterations = 60 # the number of times to train the function # the derivative of the error function (x ** 4 = the power of 4 or x^4) def slope_at_given_x_value(x): return 5 * x ** 4 - 6 * x ** 2 # Move X to the right or left depending on the slope of the error function x = [current_x] for i in range(num_iterations): previous_x = current_x current_x += -learning_rate * slope_at_given_x_value(previous_x) x.append(current_x) #print(previous_x) print(\"The local minimum occurs at %f, it is %f\" % (current_x, current_x ** 5 - 2 * current_x ** 3 - 2)) The local minimum occurs at 1.092837, it is -3.051583 import numpy as np import matplotlib.pyplot as plt plt.plot(x, marker='*') plt.show() 沿梯度（斜率）的反方向移动，这就是“梯度下降法”。如上图所示，不管初始化值设为什么，在迭代过程只会越来越接近目标值，而不会偏离目标值，这就是梯度下降法的魅力。   上面这张图是表示的是一个一元函数搜索极值的问题，未必能很好展示梯度下降法的魅力，你再返回去看上面那张“势能梯度图”，那是一个二元函数搜索极值的过程。左边的搜索路径很简洁，而右边的搜索路径，尽管因为初始值的设定，导致它的路径很曲折，但是，你有没有发现，它的每一次迭代事实上离目标都更近一步。我想，这就是梯度下降法的优点吧！ 注：这段代码是一元函数求极值，如果是二元函数，则需要同时满足两个分量的偏导数的值为零，下面的线性回归程序算的就是二元偏导数。 通过组合最小二乘法和梯度下降法，你可以得到线性回归，如下： # Price of wheat/kg and the average price of bread wheat_and_bread = [[0.5,5],[0.6,5.5],[0.8,6],[1.1,6.8],[1.4,7]] def step_gradient(b_current, m_current, points, learningRate): b_gradient = 0 m_gradient = 0 N = float(len(points)) for i in range(0, len(points)): x = points[i][0] y = points[i][1] b_gradient += -(2/N) * (y -((m_current * x) + b_current)) m_gradient += -(2/N) * x * (y -((m_current * x) + b_current)) new_b = b_current -(learningRate * b_gradient) new_m = m_current -(learningRate * m_gradient) return [new_b, new_m] def gradient_descent_runner(points, starting_b, starting_m, learning_rate, num_iterations): b = starting_b m = starting_m for i in range(num_iterations): b, m = step_gradient(b, m, points, learning_rate) return [b, m] gradient_descent_runner(wheat_and_bread, 1, 1, 0.01, 1000) # [3.853945094921183, 2.4895803107016445] 上面这个程序的核心思想就是：在内层迭代的过程中，算出每一步误差函数相当于 m 和 b 的偏导数（梯度），然后沿梯度的反方向调整 m 和 b ；外层迭代执行梯度下降法，逐步逼近偏导数等于0的点。   其中需要注意偏导数的近似计算公式，已知误差函数 E(m,b)=1N⋅∑i=0N[yi−(m⋅xi+b)]2 E(m, b)=\\frac{1}{N} \\cdot \\sum_{i=0}^{N}\\left[y_{i}-\\left(m \\cdot x_{i}+b\\right)\\right]^{2} E(m,b)=​N​​1​​⋅​i=0​∑​N​​[y​i​​−(m⋅x​i​​+b)]​2​​  即各点与拟合直线的距离的平方和，再做算术平均。然后可以计算偏导数为 ∂E∂m=−2N⋅xi⋅∑i=0N[yi−(m⋅xi+b)]∂E∂b=−2N⋅∑i=0N[yi−(m⋅xi+b)] \\begin{aligned} \\frac{\\partial E}{\\partial m}&=-\\frac{2}{N} \\cdot x_{i} \\cdot \\sum_{i=0}^{N}\\left[y_{i}-\\left(m \\cdot x_{i}+b\\right)\\right] \\\\ \\frac{\\partial E}{\\partial b} &=-\\frac{2}{N} \\cdot \\sum_{i=0}^{N}\\left[y_{i}-\\left(m \\cdot x_{i}+b\\right)\\right] \\end{aligned} ​​∂m​​∂E​​​​∂b​​∂E​​​​​=−​N​​2​​⋅x​i​​⋅​i=0​∑​N​​[y​i​​−(m⋅x​i​​+b)]​=−​N​​2​​⋅​i=0​∑​N​​[y​i​​−(m⋅x​i​​+b)]​​ 其中的求和公式在程序中表现为内层for循环   下面再给出拟合后的效果图 import numpy as np import matplotlib.pyplot as plt a = np.array(wheat_and_bread) plt.plot(a[:,0], a[:,1], 'ro') b,m = gradient_descent_runner(wheat_and_bread, 1, 1, 0.01, 1000) x = np.linspace(a[0,0], a[-1,0]) y = m * x + b plt.plot(x, y) plt.grid() plt.show() 对比 Numpy import numpy as np import matplotlib.pyplot as plt a = np.array(wheat_and_bread) plt.plot(a[:,0], a[:,1], 'ro') m, b = np.polyfit(a[:,0], a[:,1], 1) print([b,m]) x = np.linspace(a[0,0], a[-1,0]) y = m * x + b plt.plot(x, y) plt.grid() plt.show() # [4.1072992700729891, 2.2189781021897814] 参考 [机器学习] ML重要概念：梯度（Gradient）与梯度下降法（Gradient Descent） 梯度向量与梯度下降法 机器学习中的矩阵、向量求导 机器学习中的矩阵向量求导 Update time： 2020-07-17 "},"数学基础/数值计算基础/Jacobian矩阵和Hessian矩阵.html":{"url":"数学基础/数值计算基础/Jacobian矩阵和Hessian矩阵.html","title":"Jacobian矩阵和Hessian矩阵","keywords":"","body":"Jacobian矩阵和Hessian矩阵 在向量分析中, 雅可比矩阵是一阶偏导数以一定方式排列成的矩阵, 其行列式称为雅可比行列式. 还有, 在代数几何中, 代数曲线的雅可比量表示雅可比簇：伴随该曲线的一个代数群, 曲线可以嵌入其中. 它们全部都以数学家卡尔·雅可比(Carl Jacob, 1804年10月4日－1851年2月18日)命名； 一、雅可比矩阵 雅可比矩阵的重要性在于它体现了一个可微方程与给出点的最优线性逼近. 因此, 雅可比矩阵类似于多元函数的导数. 假设 Rn→Rm{R_n} \\to {R_m}R​n​​→R​m​​ 是一个从欧式n维空间转换到欧式m维空间的函数. {y1=f1(x1,…,xn)y2=f2(x1,…,xn)⋯ym=fn(x1,…,xn) \\left\\{\\begin{array}{l} y_{1}=f_{1}\\left(x_{1}, \\ldots, x_{n}\\right) \\\\ y_{2}=f_{2}\\left(x_{1}, \\ldots, x_{n}\\right) \\\\ \\cdots \\\\ y_{m}=f_{n}\\left(x_{1}, \\ldots, x_{n}\\right) \\end{array}\\right. ​⎩​⎪​⎪​⎨​⎪​⎪​⎧​​​y​1​​=f​1​​(x​1​​,…,x​n​​)​y​2​​=f​2​​(x​1​​,…,x​n​​)​⋯​y​m​​=f​n​​(x​1​​,…,x​n​​)​​ 这些函数的偏导数(如果存在)可以组成一个m行n列的矩阵, 这就是所谓的雅可比矩阵： J=[∂f∂x1⋯∂f∂xn]=[∂f1∂x1⋯∂f1∂xn⋮⋱⋮∂fm∂x1⋯∂fm∂xn] \\mathbf{J}=\\left[\\begin{array}{ccc} \\frac{\\partial \\mathbf{f}}{\\partial x_{1}} & \\cdots & \\frac{\\partial \\mathbf{f}}{\\partial x_{n}} \\end{array}\\right]=\\left[\\begin{array}{ccc} \\frac{\\partial f_{1}}{\\partial x_{1}} & \\cdots & \\frac{\\partial f_{1}}{\\partial x_{n}} \\\\ \\vdots & \\ddots & \\vdots \\\\ \\frac{\\partial f_{m}}{\\partial x_{1}} & \\cdots & \\frac{\\partial f_{m}}{\\partial x_{n}} \\end{array}\\right] J=[​​∂x​1​​​​∂f​​​​​⋯​​​​∂x​n​​​​∂f​​​​]=​⎣​⎢​⎢​⎢​⎡​​​​∂x​1​​​​∂f​1​​​​​⋮​​∂x​1​​​​∂f​m​​​​​​​⋯​⋱​⋯​​​​∂x​n​​​​∂f​1​​​​​⋮​​∂x​n​​​​∂f​m​​​​​​​⎦​⎥​⎥​⎥​⎤​​ 此矩阵表示为:JF(x1,…,xn){J_F}({x_1}, \\ldots ,{x_n})J​F​​(x​1​​,…,x​n​​) 如果 ppp 是 Rn{R_n}R​n​​ 中的一点, FFF 在 p{p}p点可微分, 那么在这一点的导数由 JF(p){J_F}({p})J​F​​(p) 给出(这是求该点导数最简便的方法). 在此情况下, 由 F(p)F(p)F(p) 描述的线性算子即接近点 ppp 的 FFF 的最优线性逼近, 逼近于: 与泰勒一阶展开近似 二、雅可比行列式 如果m = n, 那么 FFF 是从 n 维空间到 n 维空间的函数, 且它的雅可比矩阵是一个方块矩阵. 于是我们可以取它的行列式, 称为雅可比行列式. 在某个给定点的雅可比行列式提供了 在接近该点时的表现的重要信息. 如果连续可微函数 FFF 在 ppp 点的雅可比行列式不是零, 那么它在该点附近具有反函数. 这称为反函数定理. 更进一步, 如果 ppp 点的雅可比行列式是正数, 则 FFF 在 ppp 点的取向不变； 如果是负数, 则 FFF 的取向相反 而从雅可比行列式的绝对值, 就可以知道函数 FFF 在 ppp 点的缩放因子；这就是为什么它出现在换元积分法中. 对于取向问题可以这么理解, 例如一个物体在平面上匀速运动, 如果施加一个正方向的力 FFF , 即取向相同, 则加速运动, 类比于速度的导数加速度为正；如果施加一个反方向的力 FFF , 即取向相反, 则减速运动, 类比于速度的导数加速度为负. 三、海森Hessian矩阵 二阶导数 f′′(x)f^{\\prime \\prime}(x)f​′′​​(x) 刻画了曲率。假设有一个二次函数（实际任务中，很多函数不是二次的，但是在局部可以近似为二次函数）： 如果函数的二阶导数为零，则它是一条直线。如果梯度为 1，则当沿着负梯度的步长为 ϵ\\epsilonϵ 时，函数值减少ϵ\\epsilonϵ 。 如果函数的二阶导数为负，则函数向下弯曲。如果梯度为1，则当沿着负梯度的步长为 ϵ\\epsilonϵ 时，函数值减少的量大于 ϵ\\epsilonϵ 。 如果函数的二阶导数为正，则函数向上弯曲。如果梯度为1，则当沿着负梯度的步长为 ϵ\\epsilonϵ 时，函数值减少的量少于 ϵ\\epsilonϵ 。 在数学中, 海森矩阵(Hessian matrix或Hessian)是一个自变量为向量的实值函数的二阶偏导数组成的方块矩阵, 此函数如下： f(x1,x2…,xn) f({x_1},{x_2} \\ldots ,{x_n}) f(x​1​​,x​2​​…,x​n​​) 如果 fff 的所有二阶导数都存在, 那么 fff 的海森矩阵即： H(f)ij(x)=DiDjf(x) H{(f)_{ij}}(x) = {D_i}{D_j}f(x) H(f)​ij​​(x)=D​i​​D​j​​f(x) 其中 x=(x1,x2…,xn)x = ({x_1},{x_2} \\ldots ,{x_n})x=(x​1​​,x​2​​…,x​n​​) 即 H(f)H(f)H(f) 为: [∂2f∂x12∂2f∂x1∂x2⋯∂2f∂x1∂xn∂2f∂x2∂x1∂2f∂x22⋯∂2f∂x2∂xn⋮⋮⋱⋮∂2f∂xn∂x1∂2f∂xn∂x2⋯∂2f∂xn2] \\begin{bmatrix} \\frac{\\partial^2 f}{\\partial x_1^2} & \\frac{\\partial^2 f}{\\partial x_1\\,\\partial x_2} & \\cdots & \\frac{\\partial^2 f}{\\partial x_1\\,\\partial x_n} \\\\ \\\\ \\frac{\\partial^2 f}{\\partial x_2\\,\\partial x_1} & \\frac{\\partial^2 f}{\\partial x_2^2} & \\cdots & \\frac{\\partial^2 f}{\\partial x_2\\,\\partial x_n} \\\\ \\\\ \\vdots & \\vdots & \\ddots & \\vdots \\\\ \\\\ \\frac{\\partial^2 f}{\\partial x_n\\,\\partial x_1} & \\frac{\\partial^2 f}{\\partial x_n\\,\\partial x_2} & \\cdots & \\frac{\\partial^2 f}{\\partial x_n^2} \\end{bmatrix} ​⎣​⎢​⎢​⎢​⎢​⎢​⎢​⎢​⎢​⎢​⎢​⎢​⎢​⎢​⎢​⎡​​​​∂x​1​2​​​​∂​2​​f​​​​​∂x​2​​∂x​1​​​​∂​2​​f​​​​⋮​​​∂x​n​​∂x​1​​​​∂​2​​f​​​​​​∂x​1​​∂x​2​​​​∂​2​​f​​​​∂x​2​2​​​​∂​2​​f​​​⋮​​∂x​n​​∂x​2​​​​∂​2​​f​​​​​⋯​⋯​⋱​⋯​​​​∂x​1​​∂x​n​​​​∂​2​​f​​​​∂x​2​​∂x​n​​​​∂​2​​f​​​⋮​​∂x​n​2​​​​∂​2​​f​​​​​⎦​⎥​⎥​⎥​⎥​⎥​⎥​⎥​⎥​⎥​⎥​⎥​⎥​⎥​⎥​⎤​​ 即海森矩阵的第 iii 行 jjj 列元素为：Hi,j=∂2∂xi∂xjf(x⃗)\\mathbf{H}_{i, j}=\\frac{\\partial^{2}}{\\partial x_{i} \\partial x_{j}} f(\\vec{\\mathbf{x}})H​i,j​​=​∂x​i​​∂x​j​​​​∂​2​​​​f(​x​⃗​​) 海森矩阵被应用于牛顿法解决的大规模优化问题. 当二阶偏导是连续时，海森矩阵是对称阵，即有： H=HT\\mathbf{H}=\\mathbf{H}^{T}H=H​T​​ 对于特定方向 d⃗\\vec{\\mathrm{d}}​d​⃗​​ 上的二阶导数为：d⃗THd⃗\\vec{\\mathrm{d}}^{T} \\mathbf{H} \\vec{\\mathrm{d}}​d​⃗​​​T​​H​d​⃗​​ 。 如果 d⃗\\vec{\\mathrm{d}}​d​⃗​​ 是海森矩阵的特征向量，则该方向的二阶导数就是对应的特征值。 如果 d⃗\\vec{\\mathrm{d}}​d​⃗​​ 不是海森矩阵的特征向量，则该方向的二阶导数就是所有特征值的加权平均，权重在 (0,1)之间。且与 d⃗\\vec{\\mathrm{d}}​d​⃗​​ 夹角越小的特征向量对应的特征值具有更大的权重。 最大特征值确定了最大二阶导数，最小特征值确定最小二阶导数。 四、海森矩阵与学习率 将 f(x⃗)f(\\vec{\\mathbf{x}})f(​x​⃗​​) 在 x0⃗\\vec{\\mathbf{x_0}}​x​0​​​⃗​​ 处泰勒展开：f(x⃗)≈f(x⃗0)+(x⃗−x⃗0)Tg⃗+12(x⃗−x⃗0)TH(x⃗−x⃗0)f(\\vec{\\mathbf{x}}) \\approx f\\left(\\vec{\\mathbf{x}}_{0}\\right)+\\left(\\vec{\\mathbf{x}}-\\vec{\\mathbf{x}}_{0}\\right)^{T} \\vec{\\mathbf{g}}+\\frac{1}{2}\\left(\\vec{\\mathbf{x}}-\\vec{\\mathbf{x}}_{0}\\right)^{T} \\mathbf{H}\\left(\\vec{\\mathbf{x}}-\\vec{\\mathbf{x}}_{0}\\right)f(​x​⃗​​)≈f(​x​⃗​​​0​​)+(​x​⃗​​−​x​⃗​​​0​​)​T​​​g​⃗​​+​2​​1​​(​x​⃗​​−​x​⃗​​​0​​)​T​​H(​x​⃗​​−​x​⃗​​​0​​) 。其中： g⃗\\vec{\\mathbf{g}}​g​⃗​​ 为 x0⃗\\vec{\\mathbf{x_0}}​x​0​​​⃗​​ 处的梯度 ；HHH 为 x0⃗\\vec{\\mathbf{x_0}}​x​0​​​⃗​​ 处的海森矩阵。 根据梯度下降法：x⃗′=x⃗−ϵ∇x⃗f(x⃗)\\vec{\\mathbf{x}}^{\\prime}=\\vec{\\mathbf{x}}-\\epsilon \\nabla_{\\vec{\\mathbf{x}}} f(\\vec{\\mathbf{x}})​x​⃗​​​′​​=​x​⃗​​−ϵ∇​​x​⃗​​​​f(​x​⃗​​) 应用在点 x0⃗\\vec{\\mathbf{x_0}}​x​0​​​⃗​​ ，有：f(x⃗0−ϵg⃗)≈f(x⃗0)−ϵg⃗Tg⃗+12ϵ2g⃗THg⃗f\\left(\\vec{\\mathbf{x}}_{0}-\\epsilon \\vec{\\mathbf{g}}\\right) \\approx f\\left(\\vec{\\mathbf{x}}_{0}\\right)-\\epsilon \\vec{\\mathbf{g}}^{T} \\vec{\\mathbf{g}}+\\frac{1}{2} \\epsilon^{2} \\vec{\\mathbf{g}}^{T} \\mathbf{H} \\vec{\\mathbf{g}}f(​x​⃗​​​0​​−ϵ​g​⃗​​)≈f(​x​⃗​​​0​​)−ϵ​g​⃗​​​T​​​g​⃗​​+​2​​1​​ϵ​2​​​g​⃗​​​T​​H​g​⃗​​ 第一项代表函数在点 x0⃗\\vec{\\mathbf{x_0}}​x​0​​​⃗​​ 处的值。 第二项代表由于斜率的存在，导致函数值的变化。 第三项代表由于曲率的存在，对于函数值变化的矫正。 注意：如果 12ϵ2g⃗THg⃗\\frac{1}{2} \\epsilon^{2} \\vec{\\mathbf{g}}^{T} \\mathbf{H} \\vec{\\mathbf{g}}​2​​1​​ϵ​2​​​g​⃗​​​T​​H​g​⃗​​ 较大，则很有可能导致：沿着负梯度的方向，函数值反而增加！ 如果 g⃗THg⃗≤0\\vec{\\mathbf{g}}^{T} \\mathbf{H} \\vec{\\mathbf{g}} \\leq 0​g​⃗​​​T​​H​g​⃗​​≤0，则无论 ϵ\\epsilonϵ 取多大的值， 可以保证函数值是减小的。 如果 g⃗THg⃗>0\\vec{\\mathbf{g}}^{T} \\mathbf{H} \\vec{\\mathbf{g}} > 0​g​⃗​​​T​​H​g​⃗​​>0 ，则学习率 ϵ\\epsilonϵ 不能太大。若 ϵ\\epsilonϵ 太大则函数值增加。 根据 f(x⃗0−ϵg⃗)−f(x⃗0)0f\\left(\\vec{\\mathbf{x}}_{0}-\\epsilon \\vec{\\mathbf{g}}\\right)-f\\left(\\vec{\\mathbf{x}}_{0}\\right)f(​x​⃗​​​0​​−ϵ​g​⃗​​)−f(​x​⃗​​​0​​)0 ，则需要满足：ϵ2g⃗Tg⃗g⃗THg⃗\\epsilonϵ​​g​⃗​​​T​​H​g​⃗​​​​2​g​⃗​​​T​​​g​⃗​​​​ 。若 ϵ≥2g⃗Tg⃗g⃗THg⃗\\epsilon \\geq \\frac{2 \\vec{\\mathrm{g}}^{T} \\vec{\\mathrm{g}}}{\\vec{\\mathrm{g}}^{T} \\mathrm{H} \\vec{\\mathrm{g}}}ϵ≥​​g​⃗​​​T​​H​g​⃗​​​​2​g​⃗​​​T​​​g​⃗​​​​ ，则会导致沿着负梯度的方向函数值在增加。 考虑最速下降法，选择使得 fff 下降最快的 ϵ\\epsilonϵ ，则有：ϵ∗=argminϵ,ϵ>0f(x⃗0−ϵg⃗)\\epsilon^{*}=\\arg \\min _{\\epsilon, \\epsilon>0} f\\left(\\vec{\\mathbf{x}}_{0}-\\epsilon \\vec{\\mathbf{g}}\\right)ϵ​∗​​=argmin​ϵ,ϵ>0​​f(​x​⃗​​​0​​−ϵ​g​⃗​​) 。求解 ∂∂ϵf(x⃗0−ϵg⃗)=0\\frac{\\partial}{\\partial \\epsilon} f\\left(\\vec{\\mathbf{x}}_{0}-\\epsilon \\vec{\\mathbf{g}}\\right)=0​∂ϵ​​∂​​f(​x​⃗​​​0​​−ϵ​g​⃗​​)=0 有 ：ϵ∗=g⃗Tg⃗g⃗THg⃗\\epsilon^{*}=\\frac{\\vec{\\mathbf{g}}^{T} \\vec{\\mathbf{g}}}{\\vec{\\mathbf{g}}^{T} \\mathbf{H} \\vec{\\mathbf{g}}}ϵ​∗​​=​​g​⃗​​​T​​H​g​⃗​​​​​g​⃗​​​T​​​g​⃗​​​​ 。 根据 g⃗THg⃗>0\\vec{\\mathbf{g}}^{T} \\mathbf{H} \\vec{\\mathbf{g}} > 0​g​⃗​​​T​​H​g​⃗​​>0 ，很明显有： ：ϵ2g⃗Tg⃗g⃗THg⃗\\epsilonϵ​​g​⃗​​​T​​H​g​⃗​​​​2​g​⃗​​​T​​​g​⃗​​​​ 。 由于海森矩阵为实对称阵，因此它可以进行特征值分解。假设其特征值从大到小排列为：λ1≥λ2≥⋯≥λn\\lambda_{1} \\geq \\lambda_{2} \\geq \\cdots \\geq \\lambda_{n}λ​1​​≥λ​2​​≥⋯≥λ​n​​ 。 海森矩阵的瑞利商为：R(x⃗)=x⃗THx⃗x⃗Tx⃗,x⃗≠0⃗R(\\vec{\\mathbf{x}})=\\frac{\\vec{\\mathbf{x}}^{T} \\mathbf{H} \\vec{\\mathbf{x}}}{\\vec{\\mathbf{x}}^{T} \\vec{\\mathbf{x}}}, \\vec{\\mathbf{x}} \\neq \\vec{\\mathbf{0}}R(​x​⃗​​)=​​x​⃗​​​T​​​x​⃗​​​​​x​⃗​​​T​​H​x​⃗​​​​,​x​⃗​​≠​0​⃗​​ 。可以证明： λn≤R(x⃗)≤λ1λ1=maxx⃗≠0⃗R(x⃗)λn=minx⃗≠0⃗R(x⃗) \\begin{aligned} \\lambda_{n} & \\leq R(\\vec{\\mathbf{x}}) \\leq \\lambda_{1} \\\\ \\lambda_{1} &=\\max _{\\vec{\\mathbf{x}} \\neq \\vec{\\mathbf{0}}} R(\\vec{\\mathbf{x}}) \\\\ \\lambda_{n} &=\\min _{\\vec{\\mathbf{x}} \\neq \\vec{0}} R(\\vec{\\mathbf{x}}) \\end{aligned} ​λ​n​​​λ​1​​​λ​n​​​​​≤R(​x​⃗​​)≤λ​1​​​=​​x​⃗​​≠​0​⃗​​​max​​R(​x​⃗​​)​=​​x​⃗​​≠​0​⃗​​​min​​R(​x​⃗​​)​​ 根据 ϵ∗=g⃗Tg⃗g⃗THg⃗=1R(g⃗)\\epsilon^{*}=\\frac{\\vec{\\mathbf{g}}^{T} \\vec{\\mathbf{g}}}{\\vec{\\mathbf{g}}^{T} \\mathbf{H} \\vec{\\mathbf{g}}}=\\frac{1}{R(\\vec{\\mathbf{g}})}ϵ​∗​​=​​g​⃗​​​T​​H​g​⃗​​​​​g​⃗​​​T​​​g​⃗​​​​=​R(​g​⃗​​)​​1​​ 可知：海森矩阵决定了学习率的取值范围。最坏的情况下，梯度 g⃗\\vec{\\mathbf{g}}​g​⃗​​与海森矩阵最大特征值 λ1\\lambda_1λ​1​​ 对应的特征向量平行，则此时最优学习率为 1λ1\\frac{1}{\\lambda_{1}}​λ​1​​​​1​​ 。 五、驻点与全局极小点 满足导数为零的点（即 f′′(x)=0f^{\\prime \\prime}(x)=0f​′′​​(x)=0）称作驻点。驻点可能为下面三种类型之一： 局部极小点：在 xxx 的一个邻域内，该点的值最小。 局部极大点：在 xxx 的一个邻域内，该点的值最大。 鞍点：既不是局部极小，也不是局部极大。 全局极小点： x∗=argminxf(x)x^{*}=\\arg \\min _{x} f(x)x​∗​​=argmin​x​​f(x) 全局极小点可能有一个或者多个。 在深度学习中，目标函数很可能具有非常多的局部极小点，以及许多位于平坦区域的鞍点。这使得优化非常不利。 因此通常选取一个非常低的目标函数值，而不一定要是全局最小值。 二阶导数可以配合一阶导数来决定驻点的类型： 局部极小点： f′(x)=0,f′′(x)>0f^{\\prime}(x)=0, f^{\\prime \\prime}(x)>0f​′​​(x)=0,f​′′​​(x)>0 。 局部极大点： f′(x)=0,f′′(x)0f^{\\prime}(x)=0, f^{\\prime \\prime}(x)f​′​​(x)=0,f​′′​​(x)0。 f′(x)=0,f′′(x)=0f^{\\prime}(x)=0, f^{\\prime \\prime}(x)=0f​′​​(x)=0,f​′′​​(x)=0 ：驻点的类型可能为任意三者之一。 对于多维的情况类似有： 局部极小点：∇xf(x)=0\\nabla_{\\mathbf{x}}f(\\mathbf{x})=0∇​x​​f(x)=0 ，且海森矩阵为正定的（即所有的特征值都是正的）,当海森矩阵为正定时，任意方向的二阶偏导数都是正的。 局部极大点：∇xf(x)=0\\nabla_{\\mathbf{x}} f(\\mathbf{x})=0∇​x​​f(x)=0，且海森矩阵为负定的（即所有的特征值都是负的）。 当海森矩阵为负定时，任意方向的二阶偏导数都是负的。 ∇xf(x)=0\\nabla_{\\mathbf{x}} f(\\mathbf{x})=0∇​x​​f(x)=0 ，且海森矩阵的特征值中至少一个正值、至少一个负值时，为鞍点。 当海森矩阵非上述情况时，驻点类型无法判断。 下图为 f(x)=x12−x22f({\\mathbf{x}})=x_{1}^{2}-x_{2}^{2}f(x)=x​1​2​​−x​2​2​​ 在原点附近的等值线。其海森矩阵为一正一负。 沿着 x1x_1x​1​​ 方向，曲线向上弯曲；沿着 x2x_2x​2​​ 方向，曲线向下弯曲。 鞍点就是在一个横截面内的局部极小值，另一个横截面内的局部极大值。 参考 数值计算:三、二阶导数与海森矩阵 Jacobian矩阵和Hessian矩阵 Update time： 2020-07-17 "},"数学基础/数值计算基础/牛顿法与拟牛顿法.html":{"url":"数学基础/数值计算基础/牛顿法与拟牛顿法.html","title":"牛顿法与拟牛顿法","keywords":"","body":"牛顿法与拟牛顿法 牛顿法是数值优化算法中的大家族，她和她的改进型在很多实际问题中得到了应用。在机器学习中，牛顿法是和梯度下降法地位相当的的主要优化算法。 牛顿法不仅可以用来求解函数的极值问题，还可以用来求解方程的根，二者在本质上是一个问题，因为求解函数极值的思路是寻找导数为 0 的点，这就是求解方程。在本文中，我们介绍的是求解函数极值的牛顿法。 和梯度下降法一样，牛顿法也是寻找导数为 0 的点，同样是一种迭代法。核心思想是在某点处用二次函数来近似目标函数，得到导数为 0 的方程，求解该方程，得到下一个迭代点。因为是用二次函数近似，因此可能会有误差，需要反复这样迭代，直到到达导数为 0 的点处。下面我们开始具体的推导，先考虑一元函数的情况，然后推广到多元函数。 一元函数的情况 为了能让大家更好的理解推导过程的原理，首先考虑一元函数的情况。根据一元函数的泰勒展开公式，我们对目标函数在 x0x_0x​0​​ 点处做泰勒展开，有： f(x)=f(x0)+f′(x0)(x−x0)+12f′′(x0)(x−x0)2+…+1n!f(n)(x0)(x−x0)n… f(x)=f\\left(x_{0}\\right)+f'\\left(x_{0}\\right)\\left(x-x_{0}\\right)+\\frac{1}{2} f^{''}\\left(x_{0}\\right)\\left(x-x_{0}\\right)^{2}+\\ldots+\\frac{1}{n !} f^{(n)}\\left(x_{0}\\right)\\left(x-x_{0}\\right)^{n} \\ldots f(x)=f(x​0​​)+f​′​​(x​0​​)(x−x​0​​)+​2​​1​​f​​′′​​​​(x​0​​)(x−x​0​​)​2​​+…+​n!​​1​​f​(n)​​(x​0​​)(x−x​0​​)​n​​… 如果忽略 2 次以上的项，则有： f(x)=f(x0)+f′(x0)(x−x0)+12f′′(x0)(x−x0)2 f(x)=f\\left(x_{0}\\right)+f'\\left(x_{0}\\right)\\left(x-x_{0}\\right)+\\frac{1}{2} f^{''}\\left(x_{0}\\right)\\left(x-x_{0}\\right)^{2} f(x)=f(x​0​​)+f​′​​(x​0​​)(x−x​0​​)+​2​​1​​f​​′′​​​​(x​0​​)(x−x​0​​)​2​​ 现在我们在 x0x_0x​0​​ 点处，要以它为基础，找到导数为 0 的点，即导数为 0。对上面等式两边同时求导，并令导数为 0，可以得到下面的方程： f′(x)=f′(x0)+f′′(x0)(x−x0)=0 f^{\\prime}(x)=f^{\\prime}\\left(x_{0}\\right)+f^{\\prime \\prime}\\left(x_{0}\\right)\\left(x-x_{0}\\right)=0 f​′​​(x)=f​′​​(x​0​​)+f​′′​​(x​0​​)(x−x​0​​)=0 可以解得： x=x0−f′(x0)f′′(x0) x=x_{0}-\\frac{f^{\\prime}\\left(x_{0}\\right)}{f^{\\prime \\prime}\\left(x_{0}\\right)} x=x​0​​−​f​′′​​(x​0​​)​​f​′​​(x​0​​)​​ 这样我们就得到了下一点的位置，从而走到 x1x_1x​1​​。接下来重复这个过程，直到到达导数为 0 的点，由此得到牛顿法的迭代公式： xt+1=xt−f′(xt)f′′(xt) x_{t+1}=x_{t}-\\frac{f^{\\prime}\\left(x_{t}\\right)}{f^{\\prime \\prime}\\left(x_{t}\\right)} x​t+1​​=x​t​​−​f​′′​​(x​t​​)​​f​′​​(x​t​​)​​ 给定初始迭代点 x0 x_0 x​0​​ ，反复用上面的公式进行迭代，直到达到导数为 0 的点或者达到最大迭代次数。 多元函数的情况 无约束最优化问题 minx∈Rnf(x) \\min _{x \\in \\mathbf{R}^{n}} f(x) ​x∈R​n​​​min​​f(x) 其中 x∗x^{*}x​∗​​ 为目标函数的极小点。 设 f(x)f(x)f(x) 具有二阶连续偏导数，若第 kkk 次迭代值为 xkx^{k}x​k​​ ，则可将 f(x)f(x)f(x) 在 xkx^{k}x​k​​ 附近进行二阶泰勒展开 f(x)=f(x(k))+gkT(x−x(k))+12(x−x(k))TH(x(k))(x−x(x)) f(x)=f\\left(x^{(k)}\\right)+g_{k}^{T}\\left(x-x^{(k)}\\right)+\\frac{1}{2}\\left(x-x^{(k)}\\right)^{T} H\\left(x^{(k)}\\right)\\left(x-x^{(x)}\\right) f(x)=f(x​(k)​​)+g​k​T​​(x−x​(k)​​)+​2​​1​​(x−x​(k)​​)​T​​H(x​(k)​​)(x−x​(x)​​) 其中 gk=g(x(k))=∇f(x(k))g_{k}=g\\left(x^{(k)}\\right)=\\nabla f\\left(x^{(k)}\\right)g​k​​=g(x​(k)​​)=∇f(x​(k)​​) 是 f(x)f(x)f(x) 的梯度向量在点 xkx^{k}x​k​​ 的值，H(x(k))H\\left(x^{(k)}\\right)H(x​(k)​​) 是 f(x)f(x)f(x) 得黑塞矩阵： H(x)=[∂2f∂xi∂xj]n×n H(x)=\\left[\\frac{\\partial^{2} f}{\\partial x_{i} \\partial x_{j}}\\right]_{n \\times n} H(x)=[​∂x​i​​∂x​j​​​​∂​2​​f​​]​n×n​​ 在点 xkx^{k}x​k​​ 的值。 函数 f(x)f(x)f(x) 有极值的必要条件是在极值点处一阶导数为0，即梯度向量为0。特别的当 H(x(k))H\\left(x^{(k)}\\right)H(x​(k)​​) 是正定矩阵时，函数 的 f(x)f(x)f(x) 极值为极小值。 牛顿法利用极小点得必要条件： ∇f(x)=0 \\nabla f(x)=0 ∇f(x)=0 每次迭代从点 xkx^{k}x​k​​ 开始，求目标函数得极小点，作为第 k+1k+1k+1 次迭代值 xk+1x^{k+1}x​k+1​​，具体地，假设 xk+1x^{k+1}x​k+1​​ 满足： ∇f(x(k+1))=0 \\nabla f\\left(x^{(k+1)}\\right)=0 ∇f(x​(k+1)​​)=0 由 f(x)=f(x(k))+gkT(x−x(k))+12(x−x(k))TH(x(k))(x−x(x))f(x)=f\\left(x^{(k)}\\right)+g_{k}^{T}\\left(x-x^{(k)}\\right)+\\frac{1}{2}\\left(x-x^{(k)}\\right)^{T} H\\left(x^{(k)}\\right)\\left(x-x^{(x)}\\right)f(x)=f(x​(k)​​)+g​k​T​​(x−x​(k)​​)+​2​​1​​(x−x​(k)​​)​T​​H(x​(k)​​)(x−x​(x)​​) ，根据二阶泰勒展开，对 ∇f(x)\\nabla f(x)∇f(x) 在 xkx^{k}x​k​​ 进行展开得（也可以对上述泰勒公式再进行求导）得： ∇f(x)=gk+Hk(x−x(k)) \\nabla f(x)=g_{k}+H_{k}\\left(x-x^{(k)}\\right) ∇f(x)=g​k​​+H​k​​(x−x​(k)​​) 其中 Hk=H(x(k))H_{k}=H\\left(x^{(k)}\\right)H​k​​=H(x​(k)​​) ，这样, ∇f(x(k+1))=0\\nabla f\\left(x^{(k+1)}\\right)=0∇f(x​(k+1)​​)=0 成为： gk+Hk(x(k+1)−x(k))=0 g_{k}+H_{k}\\left(x^{(k+1)}-x^{(k)}\\right)=0 g​k​​+H​k​​(x​(k+1)​​−x​(k)​​)=0 因此， x(k+1)=x(k)−Hk−1gk x^{(k+1)}=x^{(k)}-H_{k}^{-1} g_{k} x​(k+1)​​=x​(k)​​−H​k​−1​​g​k​​ 或者： x(k+1)=x(k)+pk x^{(k+1)}=x^{(k)}+p_{k} x​(k+1)​​=x​(k)​​+p​k​​ 其中， Hkpk=−gk H_{k} p_{k}=-g_{k} H​k​​p​k​​=−g​k​​ 式 x(k+1)=x(k)−Hk−1gkx^{(k+1)}=x^{(k)}-H_{k}^{-1} g_{k}x​(k+1)​​=x​(k)​​−H​k​−1​​g​k​​ 作为迭代公式得算法就是牛顿法 算法步骤 牛顿法与梯度下降法 梯度下降法和牛顿法相比，两者都是迭代求解，不过梯度下降法是梯度求解，而牛顿法是用二阶的海森矩阵的逆矩阵求解。相对而言，使用牛顿法收敛更快（迭代更少次数）。但是每次迭代的时间比梯度下降法长。 梯度下降法：x(k+1)=x(k)−λ∇f(xk)x_{(k+1)}=x_{(k)}-\\lambda \\nabla f\\left(x_{k}\\right)x​(k+1)​​=x​(k)​​−λ∇f(x​k​​) 牛顿法：x(k+1)=x(k)−λ(H(k))−1∇f(xk)x_{(k+1)}=x_{(k)}-\\lambda\\left(H_{(k)}\\right)^{-1} \\nabla f\\left(x_{k}\\right)x​(k+1)​​=x​(k)​​−λ(H​(k)​​)​−1​​∇f(x​k​​) 梯度下降法中，每一次 x\\mathbf{x}x 增加的方向一定是梯度相反的方向 −ϵk∇k-\\epsilon_{k} \\nabla_{k}−ϵ​k​​∇​k​​。增加的幅度由 ϵk\\epsilon_{k}ϵ​k​​ 决定，若跨度过大容易引发震荡。 而牛顿法中，每一次 x\\mathbf{x}x 增加的方向是梯度增速最大的反方向 −Hk−1∇k-\\mathbf{H}_{k}^{-1} \\nabla_{k}−H​k​−1​​∇​k​​（它通常情况下与梯度不共线）。增加的幅度已经包含在 Hk−1\\mathbf{H}_{k}^{-1} H​k​−1​​ 中（也可以乘以学习率作为幅度的系数）。 对于不带约束条件的问题，我们可以将x的初始值设定为任意值，最简单的，可以设置为全0的向量。迭代终止的判定规则和梯度下降法相同，是检查梯度是否接近于0。 面临的问题 牛顿法并不能保证每一步迭代时函数值下降，也不保证一定收敛。为此，提出了一些补救措施，其中的一种是直线搜索（line search）技术，即搜索最优步长。具体做法是让 γ\\gammaγ 取一些典型的离散值，如0.0001,0.001,0.01等，比较取哪个值时函数值下降最快，作为最优步长。 和梯度下降法相比牛顿法有更快的收敛速度，但每一步迭代的成本也更高。在每次迭代中，除了要计算梯度向量还要计算Hessian矩阵，并求解Hessian矩阵的逆矩阵。 牛顿法面临的另外一个问题是Hessian矩阵可能不可逆，从而导致这种方法失效。此外，求解Hessian矩阵的逆矩阵或者求解线性方程组计算量大，需要耗费大量的时间。为此，提出了拟牛顿法这种改进方案 除此之外，牛顿法在每次迭代时序列xi可能不会收敛到一个最优解，它甚至不能保证函数值会按照这个序列递减。解决第一个问题可以通过调整牛顿方向的步长来实现，目前常用的方法有两种：直线搜索和可信区域法。 拟牛顿法 牛顿法在每次迭代时需要计算出Hessian矩阵，然后求解一个以该矩阵为系数矩阵的线性方程组，这非常耗时，另外Hessian矩阵可能不可逆。为此提出了一些改进的方法，典型的代表是拟牛顿法（Quasi-Newton）。 在牛顿法的迭代中，需要计算海森矩阵的逆矩阵 ，这一计算比较复杂，考虑用一个 阶矩阵 来近似代替 。这就是拟牛顿法的基本想法。 先看下牛顿法迭代中海森矩阵 Hk{H_{k}}H​k​​ 满足的条件。首先 Hk{H_{k}}H​k​​满足以下关系： 在公式 ∇f(x)=gk+Hk(x−x(k))\\nabla f(x)=g_{k}+H_{k}\\left(x-x^{(k)}\\right)∇f(x)=g​k​​+H​k​​(x−x​(k)​​) 中，取 x=x(k+1)x=x^{(k+1)}x=x​(k+1)​​ 既得： gk+1−gk=Hk(x(k+1)−x(k)) g_{k+1}-g_{k}=H_{k}\\left(x^{(k+1)}-x^{(k)}\\right) g​k+1​​−g​k​​=H​k​​(x​(k+1)​​−x​(k)​​) 记 yk=gk+1−gk,δk=x(k+1)−x(k)y_{k}=g_{k+1}-g_{k}, \\delta_{k}=x^{(k+1)}-x^{(k)}y​k​​=g​k+1​​−g​k​​,δ​k​​=x​(k+1)​​−x​(k)​​ ，则 yk=Hkδk y_{k}=H_{k} \\delta_{k} y​k​​=H​k​​δ​k​​ 或 Hk−1yk=δk H_{k}^{-1} y_{k}=\\delta_{k} H​k​−1​​y​k​​=δ​k​​ 上式称为拟牛顿条件 其次，如果 Hk{H_{k}}H​k​​ 是正定的（ Hk−1{H_{k}^{-1}}H​k​−1​​也是正定的），那么保证牛顿法的搜索方向 pkp_{k}p​k​​ 是下降方向。这是因为搜索方向是 pk=−Hk−1gkp_{k}=-H_{k}^{-1} g_{k}p​k​​=−H​k​−1​​g​k​​ ，由 x(k+1)=x(k)−Hk−1gkx^{(k+1)}=x^{(k)}-H_{k}^{-1} g_{k}x​(k+1)​​=x​(k)​​−H​k​−1​​g​k​​ 有： x=x(k)+λpk=x(k)−λHk−1gk x=x^{(k)}+\\lambda p_{k}=x^{(k)}-\\lambda H_{k}^{-1} g_{k} x=x​(k)​​+λp​k​​=x​(k)​​−λH​k​−1​​g​k​​ 所以 f(x)f(x)f(x) 在 xkx^{k}x​k​​ 得泰勒展开式 ： f(x)=f(x(k))+gkT(x−x(k))+12(x−x(k))TH(x(k))(x−x(k)) f(x)=f\\left(x^{(k)}\\right)+g_{k}^{\\mathrm{T}}\\left(x-x^{(k)}\\right)+\\frac{1}{2}\\left(x-x^{(k)}\\right)^{\\mathrm{T}} H\\left(x^{(k)}\\right)\\left(x-x^{(k)}\\right) f(x)=f(x​(k)​​)+g​k​T​​(x−x​(k)​​)+​2​​1​​(x−x​(k)​​)​T​​H(x​(k)​​)(x−x​(k)​​) 可以近似写成： f(x)=f(x(k))−λgkTHk−1gk f(x)=f\\left(x^{(k)}\\right)-\\lambda g_{k}^{\\mathrm{T}} H_{k}^{-1} g_{k} f(x)=f(x​(k)​​)−λg​k​T​​H​k​−1​​g​k​​ 因 Hk−1{H_{k}^{-1}}H​k​−1​​ 正定，故有 gkTHk−1gk>0g_{k}^{\\mathrm{T}} H_{k}^{-1} g_{k}>0g​k​T​​H​k​−1​​g​k​​>0 ， 。当 为一个充分小的正数时，总有 f(x)f(x(k))f(x)f(x)f(x​(k)​​) ，也就是说 是下降方向。 因此拟牛顿法将 Gk{G_{k}}G​k​​ 作为 Hk−1{H_{k}^{-1}}H​k​−1​​ 近似。要求 满 Gk{G_{k}}G​k​​ 足同样的条件。首先，每次迭代矩阵 Gk{G_{k}}G​k​​ 是正定的。同时， Gk{G_{k}}G​k​​ 满足下面的拟牛顿条件： Gk+1yk=δk G_{k+1} y_{k}=\\delta_{k} G​k+1​​y​k​​=δ​k​​ 按照拟牛顿条件，在每次迭代中可以选择更新矩阵 Gk+1{G_{k+1}}G​k+1​​ Gk+1=Gk+ΔGk G_{k+1}=G_{k}+\\Delta G_{k} G​k+1​​=G​k​​+ΔG​k​​ 根据此条件，构造出了多种拟牛顿法，典型的有 DFP 算法、BFGS算法、L-BFGS算法等， BFGS algorithm BFGS（Broyden-Fletcher-Goldfarb-Shanno） BFGS 算法是最流行得拟牛顿算法。 可以考虑用 GkG_kG​k​​ 逼近海森矩阵得逆矩阵 H−1H^{-1}H​−1​​，也可以考虑用 BkB_kB​k​​ 逼近海森矩阵。 这时，相应得拟牛顿条件是 Bk+1δk=yk B_{k+1} \\delta_{k}=y_{k} B​k+1​​δ​k​​=y​k​​ 可以用到同样得方法得到另一个迭代公式。首先令 Bk+1=Bk+Pk+QkBk+1δk=Bkδk+Pkδk+Qkδk \\begin{array}{c} B_{k+1}=B_{k}+P_{k}+Q_{k} \\\\ B_{k+1} \\delta_{k}=B_{k} \\delta_{k}+P_{k} \\delta_{k}+Q_{k} \\delta_{k} \\end{array} ​B​k+1​​=B​k​​+P​k​​+Q​k​​​B​k+1​​δ​k​​=B​k​​δ​k​​+P​k​​δ​k​​+Q​k​​δ​k​​​​ 考虑使 PkP_kP​k​​ 和 QkQ_kQ​k​​ 满足： Pkδk=ykQkδk=−Bkδk \\begin{array}{c} P_{k} \\delta_{k}=y_{k} \\\\ Q_{k} \\delta_{k}=-B_{k} \\delta_{k} \\end{array} ​P​k​​δ​k​​=y​k​​​Q​k​​δ​k​​=−B​k​​δ​k​​​​ 找出适合条件得 PkP_kP​k​​ 和 QkQ_kQ​k​​ ，得到 BFGS 算法矩阵 Bk+1B_{k+1}B​k+1​​ 得迭代公式： Bk+1=Bk+ykykTykTδk−BkδkδkTBkδkTBkδk B_{k+1}=B_{k}+\\frac{y_{k} y_{k}^{\\mathrm{T}}}{y_{k}^{\\mathrm{T}} \\delta_{k}}-\\frac{B_{k} \\delta_{k} \\delta_{k}^{\\mathrm{T}} B_{k}}{\\delta_{k}^{\\mathrm{T}} B_{k} \\delta_{k}} B​k+1​​=B​k​​+​y​k​T​​δ​k​​​​y​k​​y​k​T​​​​−​δ​k​T​​B​k​​δ​k​​​​B​k​​δ​k​​δ​k​T​​B​k​​​​ 可以证明，如果初始矩阵 B0B_0B​0​​ 是正定的，则迭代过程中的每个矩阵 BkB_kB​k​​ 都是正定的。 BFGS 算法流程 参考 最优化算法之牛顿法 理解牛顿法 牛顿法与Hessian矩阵 最优化算法之牛顿法 Update time： 2020-07-17 "}}